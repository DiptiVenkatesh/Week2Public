{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Machine Learning with Signals Data\n\nIn this notebook we will be start using some of our machine learning techniques on signals data (audio).  We will be using a dataset consisting of recordings of spoken digits (0-9).  We will start by extracting some features from the signals and then apply our \"traditional\" ML algorithms, just as we did with our tabular data.  (In a future notebook we will see how to use ANNs to avoid needing to extract features.)<br><br>\n\nIf you are interested in playing around with more audio data like this, we encourage you to check out [UrbanSound](https://urbansounddataset.weebly.com).",
   "metadata": {
    "id": "9XZEvoDUdSKh",
    "colab_type": "text",
    "cell_id": "00000-bffe9e2f-9c8b-4835-b8a3-890c9ec05b8e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3MIrMPcldSQZ",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00001-b52fe408-d22d-4d8c-8d86-ab18086c0659",
    "deepnote_cell_type": "code"
   },
   "source": "# Install Graphvis library if it is not already installed\n!pip install graphviz \n\n# Clone the audio data repository (should cleanly fail if already downloaded)\n!git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git\n\n# Define the path to where the data are downloaded\ndata_dir = '/content/free-spoken-digit-dataset/recordings'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vpJNjNBE8jiz",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00002-c99397a3-ba9a-4cbf-9908-4e5d9f3592a3",
    "deepnote_cell_type": "code"
   },
   "source": "# Import basic system and numerical packages\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\n# Import statistical and signal processing functions\nfrom scipy import signal\nimport scipy.stats.mstats as mstats\nfrom sklearn import metrics\n\n# Import the scikit-learn functions and models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\n\n# Import audio i/o and playing functions\nimport scipy.io.wavfile\nfrom IPython.display import Audio\n\n# Import plotting functionality\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport graphviz\nimport seaborn as sns\n\n# Setup the plotting preferencs\n%matplotlib inline\nmatplotlib.rcParams.update({'font.size': 16})",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "StOtTzNzZHge",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00003-5e40f668-5c4c-4b74-88b3-0911f708ec85",
    "deepnote_cell_type": "code"
   },
   "source": "# NOTE:  The sample resolution is the number of bits per sample and determines\n# the number of gradations in amplitude. More info can be found here:\n# http://www.asel.udel.edu/speech/tutorials/instrument/sam_res.html\n\n# Define a function to display the pre-computed fourier transform of a signal\ndef plot_FT(ft, f_s):\n    \"\"\"\n    Plots the Fourier Transform frequency spectrum\n    :param ft: output of Fourier Transform (i.e., np.fft.fft())\n    :param f_s: sampling frequency (or `sampling rate`) (in Hz)\n    \"\"\"\n    # Generate the frequencies associated with the fourier transform values\n    num_samples = len(ft)\n    freqs = np.linspace(0, f_s, num_samples)\n\n    # Plot and label the fourier transform amplitude values\n    plt.plot(freqs[0:num_samples//2],\n             np.abs(ft[0:num_samples//2]) * (2/num_samples))\n    plt.ylabel(\"Amplitude\")\n    plt.xlabel(\"Frequency [Hz]\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\nLet's take a look at one of the files and see what it \"looks\" like.",
   "metadata": {
    "id": "RggupxR1iHuI",
    "colab_type": "text",
    "cell_id": "00004-f17db5ed-a8c9-4485-8c9a-0afa0b2e5778",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vHkeAELlXEKq",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00005-20b04e16-efb0-4c65-b6d5-09941c56ac84",
    "deepnote_cell_type": "code"
   },
   "source": "# Read in an audio file: count number of samples and compute sample interval\nexample_file = os.path.join(data_dir,'0_jackson_0.wav')\n(sample_rate, y) = scipy.io.wavfile.read(example_file)\nnum_samples = len(y)\nsample_interval = 1.0/sample_rate\n\n# Using np.arange allows you to easily guarantee that the *time interval*\n# between samples is correct.\nt = np.arange(0, num_samples/sample_rate, sample_interval)\n\n# # Using np.linspace allows you to easily guarantee that the *number* of\n# # timestamps matches the number in the audio signal.\n# t = np.linspace(0, num_samples/sample_rate, num_samples, endpoint=False)\n\n# Compute the fourier transform of the audio signal\nft = np.fft.fft(y)\n\n# Compute the spectrogram of the audo signal\nsample_freq, segment_time, spec_data = signal.spectrogram(y, sample_rate)\n\n# Display the results to the user\nplt.figure(figsize=(10, 20))\n\n# Plot the original signal\nplt.subplot(4, 1, 1)\nplt.plot(t, y)\nplt.xlabel('Time (sec)')\nplt.title('Signal (time domain): {}'.format(os.path.basename(example_file)))\n\n# Display the fourier transform of the signal\nplt.subplot(4, 1, 2)\nplot_FT(ft, sample_rate)\nplt.title('Fourier Transform')\n\n# Display the spectrogram of the signal\n# (Allow Matplotlib to compute the spectrogram for you) \nplt.subplot(4, 1, 3)\nplt.specgram(y, Fs=sample_rate)\nplt.xlabel('Time (sec)')\nplt.ylabel('Frequency (Hz)')\nplt.title('Spectrogram (Matplotlib)')\n\n# Display the spectrogram of the signal\n# (This is the pre-computed version from the scipy.signal.spectrogram function)\nspec_data_dB = 10*np.log10(spec_data)\nplt.subplot(4, 1, 4)\nplt.pcolormesh(segment_time, sample_freq, spec_data_dB, vmin=0, vmax=50)\nplt.xlabel('Time (sec)')\nplt.ylabel('Frequency (Hz)')\nplt.title('Spectogram (Scipy)')\n\n# Increase padding between plots to leave room for titles and show the plots\nplt.subplots_adjust(hspace=0.6)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ciiJFa-aXq8b",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00006-5d48298e-3dec-43e9-83dd-a5a90d3e50e3",
    "deepnote_cell_type": "code"
   },
   "source": "Audio(data=y, rate=sample_rate)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Extraction\nLet's extract some features that are useful in signals processing.",
   "metadata": {
    "id": "jPEORGagajex",
    "colab_type": "text",
    "cell_id": "00007-036026ea-9f92-4a4a-a433-055e8dbfab2b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Spectral Centroid\n\nThe [**spectral centroid**](https://en.wikipedia.org/wiki/Spectral_centroid) indicates at which frequency the energy of a spectrum is centered upon. This is like a weighted mean:\n$$f_c=\\frac{\\sum_kA(k)f(k)}{\\sum_kA(k)}$$\n \nwhere $A(k)$ is the spectral magnitude at frequency bin $k$,  $f(k)$ is the frequency at bin $k$\n .",
   "metadata": {
    "id": "p4pwjmMHcjk5",
    "colab_type": "text",
    "cell_id": "00008-17efd26b-ca35-49ec-98b4-6de4ac7a4bb6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ThhUeXOCYXAN",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00009-94c34c87-2247-46fe-918d-a92a5a69dec9",
    "deepnote_cell_type": "code"
   },
   "source": "def spectral_centroid(ft, f_s):\n    \"\"\"\n    Computes the spectral centroid from the FT of a signal\n\n    :param ft: output of Fourier Transform (i.e., np.fft.fft())\n    :param f_s: sampling frequency (or `sampling rate`) (in Hz)\n    \"\"\"\n    # Generate the frequencies associated with the fourier transform values\n    num_samples = len(ft)\n    freqs = np.linspace(0, f_s, num_samples)\n\n    # Grab the magnitude of the relevant part of the fourier transform values\n    freqs = freqs[0:num_samples//2]\n    magnitude = np.abs(ft[0:num_samples//2]) * (2/num_samples)\n    \n    # Compute the weighted frequency to get the specral centroid\n    spec_centroid = np.sum(magnitude*freqs)/np.sum(magnitude)\n\n    return spec_centroid",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Spectral Flatness\n\nThe [**spectral flatness**](https://en.wikipedia.org/wiki/Spectral_flatness), also known as Wiener entropy, is a measure used in digital signal processing to characterize an audio spectrum. Spectral flatness is typically measured in decibels, and provides a way to quantify how noise-like a sound is, as opposed to being tone-like.",
   "metadata": {
    "id": "POEqXNK7lbUu",
    "colab_type": "text",
    "cell_id": "00010-422c4460-7163-40f0-9322-eae004ec08a3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LrpDbTp8bZWR",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00011-3a79651e-09ae-49ae-8c13-a9432b7841a9",
    "deepnote_cell_type": "code"
   },
   "source": "def spectral_flatness(ft):\n    \"\"\"\n    Computes the spectral flatness of the FT of a signal\n    \n    :param ft: output of Fourier Transform (i.e., np.fft.fft())\n    \"\"\"\n    # Grab the magnitude of the relevant part of the fourier transform values\n    num_samples = len(ft)\n    magnitude = abs(ft[0:num_samples//2]) * (2/num_samples)\n \n    # Spectral flatness is simply the geometric mean of the FT magnitude\n    # divided by the arithmetic mean of the FT magnitude\n    spec_flatness = mstats.gmean(magnitude)/np.mean(magnitude)\n\n    return spec_flatness",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Maximum Frequency Value\nThe frequency at maximum altitude isn't necessarily a particularly informative feature, but it's easy to extract.",
   "metadata": {
    "id": "blizQbTb8SU3",
    "colab_type": "text",
    "cell_id": "00012-553813ae-e792-4e72-b648-7a3cabda8507",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RrrsRVURlr-m",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00013-20d7b06f-de74-455a-8638-9b31cacefff6",
    "deepnote_cell_type": "code"
   },
   "source": "def max_freq(ft, f_s):\n    \"\"\"\n    Computes the frequency with the largest magnitude in the fourier transform\n\n    :param ft: output of Fourier Transform (i.e., np.fft.fft())\n    :param f_s: sampling frequency (or `sampling rate`) (in Hz)\n    \"\"\"\n    # Generate\n    num_samples = len(ft)\n    freqs = np.linspace(0, f_s, num_samples)\n    \n    # Grab the magnitude of the relevant portion of the fourier transform. In\n    # this case we are going to ignore the `0-frequency` component in case the\n    # signal was not debiased. That frequency really contains no information.\n    freqs = freqs[1:num_samples//2]\n    mag = np.abs(ft[1:num_samples//2]) * (2/num_samples)\n\n    # Use the np.argmax function to locate the maximum magnitude of the FT\n    max_ind = np.argmax(mag)\n    max_frequency = freqs[max_ind]\n\n    return max_frequency",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Calculate hand-engineered features for single example\nThis is what our one data point looks like in our dataframe:",
   "metadata": {
    "id": "hUHeQYVok4I_",
    "colab_type": "text",
    "cell_id": "00014-e2ab9d63-75a7-4500-9904-68c5ce8990ec",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fRSK0qnli8s-",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00015-3be0a92f-267e-46ec-8da0-b0041ab8f9d0",
    "deepnote_cell_type": "code"
   },
   "source": "# We can use the regular expression functionality (`re` package) to robustly\n# parse the name of each audio file to retrieve the digit spoken, the person\n# speaking, and the trial number of that digit-speaker pair.\n#\n# The following lines create little \"string parsers\" to grab that information.\nre_digit = re.compile('\\d+_')\nre_speaker = re.compile('_[a-z]+_')\nre_trial = re.compile('_\\d.')\n\n# Use these string parsers to grab the digit, speaker, and trial from the name\n# of the example_file.\ndigit = int(re.match(re_digit, os.path.basename(example_file))[0][:-1])\nspeaker = re.search(re_speaker, os.path.basename(example_file))[0][1:-1]\ntrial = int(re.search(re_trial, os.path.basename(example_file))[0][1:-1])\n\n# Load this information into a dataframe and create columns to hold the\n# Spectral Centroid (SC)\n# Spectral Flatness (SF)\n# Maximum Frequency (MF)\ndf = pd.DataFrame(columns=['file','digit','speaker','trial','SC','SF','MF'])\n\n# Compute each of these audio features\nsc = spectral_centroid(ft, sample_rate)\nsf = spectral_flatness(ft)\nmf = max_freq(ft, sample_rate)\n\n# Add the filename, digit, speaker, trial, and audio features to the datafram\nfeature_dict = {'file':os.path.basename(example_file),'digit':digit,\n                        'speaker':speaker,'trial':trial,'SC':sc,'SF':sf,'MF':mf}\ndf = df.append(feature_dict, ignore_index=True)\ndf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load full dataset and extract features",
   "metadata": {
    "id": "-mPEZTcSkYnZ",
    "colab_type": "text",
    "cell_id": "00016-a538001b-bd05-481c-b86c-278328147e24",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C2nB-pLjoVlU",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00017-f3081d78-af3b-484e-a400-3c994c846813",
    "deepnote_cell_type": "code"
   },
   "source": "# Create a dataframe to store the entire audio database and its features\ndf = pd.DataFrame(columns=['file','digit','speaker','trial','SC','SF','MF'])\n\n# Loop over each audio file in the data directory\nfor audio_file in os.listdir(data_dir):\n    # Try to load the file and parse all of its contents.\n    # If something goes wrong, Python will execute the contents of the \"except\"\n    # codeblock.\n    try:\n        # Use the same string parsers from before and grab info from filename\n        digit = int(re.match(re_digit, audio_file)[0][:-1])\n        speaker = re.search(re_speaker, audio_file)[0][1:-1]\n        trial = int(re.search(re_trial, audio_file)[0][1:-1])\n\n        # Read in the audio file\n        full_path_to_audio_file = os.path.join(data_dir, audio_file)\n        (sample_rate, y) = scipy.io.wavfile.read(full_path_to_audio_file)\n        \n        # If the audio recording is in stereo (2-channels), just use one of them\n        if len(y.shape) == 2:\n            y = y[:,0]\n\n        # Grab the number of samples, compute the sample interval, and generate\n        # the time-stamps for each of the audio samples\n        num_samples = len(y)\n        sample_interval = 1.0/sample_rate\n        t = np.arange(0, num_samples/sample_rate, sample_interval)\n\n        # Compute the Fourier Transform of the audio signal\n        ft = np.fft.fft(y)\n\n        # Calculate the audio features to be stored in the data fram\n        sc = spectral_centroid(ft, sample_rate)\n        sf = spectral_flatness(ft)\n        mf = max_freq(ft, sample_rate)\n\n        # Add the info for this file to our dataframe\n        feature_dict = {'file':os.path.basename(audio_file),'digit':digit,\n                        'speaker':speaker,'trial':trial,'SC':sc,'SF':sf,'MF':mf}\n        df = df.append(feature_dict, ignore_index=True)\n\n    except Exception as err:\n        # Something went wrong!  =(\n        # Notife the user of the audio file that broke and what the error was\n        print(audio_file)\n        raise err",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m1-LovEVqnh2",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00018-991e0b02-9f9a-4842-8d84-014bbf6e2e90",
    "deepnote_cell_type": "code"
   },
   "source": "# Print the head of the dataframe\nprint('df.head():\\n')\nprint(df.head(15))\n\n# Also print some summary information about the dataframe\nprint('\\n\\ndf.info():\\n')\nprint(df.info())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r_k_wABdu2YI",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00019-61829778-10f0-4a2d-aec4-243a3b4e4e96",
    "deepnote_cell_type": "code"
   },
   "source": "# Visualize a pairplot of the spectral features. In this instance, color each\n# datapoint by the *person* speaking the digit to see if each *speaker* has a\n# distinguishable set of spectral characteristics (e.g., deep vs high voice)\ng = sns.pairplot(data=df, vars=['SC', 'SF', 'MF'],\n                 hue='speaker', corner=True)\nplt.suptitle('Spectral Features (colored by speaker)', x=0.5, y=1.02)\n\n# Clean up the plot appearance\nplt.gcf().tight_layout()\n\n# Repeat this plot, but now color each datapoint by the *digit* being spoken to\n# see if each *word* has a distinguishable set of spectral characteristics\n# (e.g., does the buzzing 'z' in 'zero' cause a different set of characteristics\n# than the shap 't' sound in 'two'?)\ng = sns.pairplot(data=df, vars=['SC', 'SF', 'MF'],\n                 hue='digit', corner=True)\nplt.suptitle('Spectral Features (colored by digit)', x=0.5, y=1.02)\n\n# Clean up the plot appearance\nplt.gcf().tight_layout()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Train Machine Learning Algorithm\n\nWith these example features in hand, we can build a classification tool using some of the machine learning techniques from previous notebooks.",
   "metadata": {
    "id": "nT74fm4M1eV-",
    "colab_type": "text",
    "cell_id": "00020-f3f171d9-a1b0-46a1-b0f4-1c73e28d0246",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Prepare Training Data\n\nAs a first step in this process, we specify which column of the dataframe contains the 'label' information. We also specify the complete set of features to be used in the classification process. This should generally be *all* of the columns except for the one being used as the 'label,' and it should not contain any of the columns generated from the filename because these do not actually contain information about the audio content.\n\n(Note: we use the builtin Python [set object](https://docs.python.org/3.6/library/stdtypes.html#set-types-set-frozenset) to quickly process the dataframe columns.)",
   "metadata": {
    "id": "Ea-R5qDQtDbO",
    "colab_type": "text",
    "cell_id": "00021-c87f5596-2d99-49e4-b7a3-a8b32b395064",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UdTb80ywsnIJ",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00022-cce544f6-687c-4b48-a2d5-3bebd644e925",
    "deepnote_cell_type": "code"
   },
   "source": "# Select the name of the column to be used as the desired 'label' output\nlabel = 'speaker'\n\n# Create a list of features to be used in the classification process.\n# We use the 'set' object in Python to quickly remove the unwanted columns\nfeatures = set(df.columns) - set([label, 'file', 'trial'])\nfeatures = list(features)\n\nprint('Classifying {} using...'.format(label))\nprint('features: {}'.format(features))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Our next step is to convert each of the columns with discrete values (e.g., speaker, digit, trial) into the Pandas.Categorial data type. This creates a unique numerical encoding for each entry so that string values (such as speaker names) can be easily treated as numerical values.",
   "metadata": {
    "id": "GIWpQxidwiq6",
    "colab_type": "text",
    "cell_id": "00023-21fb8994-7723-4400-9206-df6a3cc2e8f5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GnBZfexpwggY",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00024-50f50b0b-dfa2-4526-84db-50c6957d4eea",
    "deepnote_cell_type": "code"
   },
   "source": "# Convert the data in the 'speaker', 'digit', and 'trial' columns of the\n# dataframe into the 'Categorical' type.\ndf.speaker = pd.Categorical(df.speaker)\ndf.digit = pd.Categorical(df.digit)\ndf.trial = pd.Categorical(df.trial)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "If the the speaker is one of the features to be used, then we need swap out its numerical representation so that we can perform computations on it.",
   "metadata": {
    "id": "GDLNE-zWyGxk",
    "colab_type": "text",
    "cell_id": "00025-ce0873d6-21cb-4b00-b098-b85c909da718",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kwFYJAyAwp_B",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00026-7cc50261-8386-492a-94e8-e8173790144c",
    "deepnote_cell_type": "code"
   },
   "source": "# If the speaker is in the set of features used to classify the audio files,\n# then the string speaker value should be converted to its numerical version.\nif 'speaker' in features:\n    # Add a column containing the numerically encoded version of speaker name\n    df['speaker_code'] = df.speaker.cat.codes\n\n    # Replace the 'speaker' feature with the 'speaker_code' feature\n    features.remove('speaker')\n    features.append('speaker_code')\n\n    # Double check that there are not duplicate entries in the list of features\n    # by converting to a `set` object (which automatically removes duplicate\n    # entries) then convert back `list` object.\n    features = list(set(features))\n\n# Update the user on what columns and content remain in the datafram\ndf.info()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Next we split the data into training and test sets.",
   "metadata": {
    "id": "V4CH6ZfdzEbA",
    "colab_type": "text",
    "cell_id": "00027-dd2a1162-345a-44c9-a790-aa3a329bf051",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "POmoLVIe1d2k",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00028-d15034ff-2357-44c0-a083-21a8cb98b9fe",
    "deepnote_cell_type": "code"
   },
   "source": "# split X and y into training and testing sets\n# NOTE: Using convention that X = features, y = label\nX_train, X_test, y_train, y_test = train_test_split(\n    df.loc[:,features], df[label], test_size=0.25,\n    random_state=0, stratify = df[label])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Now, with the correct training features and labels, we can train a machine learning classification model. To start with, let's consider a decision tree.",
   "metadata": {
    "id": "PYx9GHfizKaS",
    "colab_type": "text",
    "cell_id": "00029-ef9ee935-d020-499e-a101-d9f56095881d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wvVI50ci1eAH",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00030-f8835f5a-f7e1-4a61-9654-786fa48cf6c3",
    "deepnote_cell_type": "code"
   },
   "source": "# Instantiate a DecisionTree model\nmodel = tree.DecisionTreeClassifier(max_depth=4)\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l5TWpNhE1d9x",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00031-a12389ec-d6e6-47be-8b25-876485e577ad",
    "deepnote_cell_type": "code"
   },
   "source": "# Convert each class name into a string representation\nclass_names = [str(c) for c in model.classes_]\n\n# Generate the data to visualize the decision tree\ndot_data = tree.export_graphviz(model, out_file=None, \n                         feature_names=features,  \n                         class_names=class_names,  \n                         filled=True, rounded=True) \n\n# Visualize the tree from that exported data\ngraph = graphviz.Source(dot_data)  \ngraph",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluate on Test Data",
   "metadata": {
    "id": "vy9xs8UU4XTq",
    "colab_type": "text",
    "cell_id": "00032-f3993d21-788d-4282-b5ef-f5db29ebb46e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P7IgU64x6QAY",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00033-8919bdf2-ae73-4205-8353-94282a00c4a5",
    "deepnote_cell_type": "code"
   },
   "source": "# Visualize the confusion matrix\ndef plot_cmatrix(cm,labels,title='Confusion Matrix'):\n    \"\"\"\n    Plot the confusion matrix for the classifier\n\n    :param cm: the actual confusion matrix\n    :param labels: the labels to add along the axes of the matrix\n    :param title: the title to place over the confusion matrix plot\n    \"\"\"\n    # Generate a new figure and axes instance\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # Display the confusion matrix (attempt to account for normalization)\n    row_totals = np.sum(cm, axis=1)\n    vmax_val = row_totals.max() if row_totals.max() > 1 else 1.0\n    ax_im = ax.imshow(cm, cmap='Reds', vmin=0, vmax=vmax_val)\n\n    # Annotate the figure\n    plt.title(title, fontsize=20)\n    fig.colorbar(ax_im)\n    ax.set_xticks(range(len(labels)))\n    ax.set_yticks(range(len(labels)))\n    ax.set_xticklabels(labels, fontsize=16, rotation=70)\n    ax.set_yticklabels(labels, fontsize=16)\n    plt.xlabel('Predicted', fontsize=16)\n    plt.ylabel('True', fontsize=16)\n    plt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q6nWEv1M4Xf9",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00034-5f6d29a2-66f6-4e52-a657-a45ad91f4bbf",
    "deepnote_cell_type": "code"
   },
   "source": "# Predict class label probabilities\nlabels=np.sort(y_test.unique())\ny_test_pred = model.predict(X_test)\n\ncm = metrics.confusion_matrix(y_test,y_test_pred, labels)\nplot_cmatrix(cm, labels)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Conclusion\n\nThis is just a tutorial on how to get this process running. The real challenge is in designing appropriate features to adequately separate out the classes. You should also consider the possibility of using *other* classification models (KNN, logistic regression, etc...).",
   "metadata": {
    "id": "h_-B4Ay05qC2",
    "colab_type": "text",
    "cell_id": "00035-db9e4a50-bafc-42aa-90e4-7f6d85f4e9ae",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ea64f2fe-fb8d-4ed3-839c-873225803319' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "SignalsML_Tutorial.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "deepnote_notebook_id": "7168c476-5e2c-4fa8-b08b-bb708c7277e9",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}